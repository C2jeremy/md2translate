{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4c9f748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported: os, re, time, sys, openai, IPython.display, tiktoken, tqdm.notebook\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 1: Setup and Imports ---\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import sys\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, Markdown # For better display in Jupyter\n",
    "import tiktoken # Added for token counting\n",
    "from tqdm.notebook import tqdm # Added for progress bar\n",
    "\n",
    "print(\"Libraries imported: os, re, time, sys, openai, IPython.display, tiktoken, tqdm.notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27f836c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI client initialized successfully for Base URL: https://api.x.ai/v1\n",
      "✅ tiktoken tokenizer ('cl100k_base') initialized for token estimation.\n",
      "   ℹ️ Note: Token counts are estimates; Grok's internal counting may differ.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 2: Configuration ---\n",
    "\n",
    "# --- Constants ---\n",
    "API_RETRY_DELAY = 5 # API call retry delay in seconds\n",
    "MAX_RETRIES = 3     # Maximum API call retries\n",
    "XAI_BASE_URL = \"https://api.x.ai/v1\" # Grok API Base URL\n",
    "\n",
    "# --- Parameters (Modify as needed) ---\n",
    "CHUNK_SIZE = 3       # Number of paragraphs per API call batch\n",
    "XAI_MODEL = \"grok-3-mini\" # Model to use for translation\n",
    "\n",
    "# --- API Key and Client Initialization ---\n",
    "XAI_API_KEY = \"xai-zdA7bOhcWyNd0FQBOL00BEsmJiPCnxllZLaxeTdWePEdFGFrIYP64K8aGDJaiLnaNUnkcuGBIGHT23FH\"\n",
    "client = None\n",
    "\n",
    "if XAI_API_KEY:\n",
    "    try:\n",
    "        client = OpenAI(\n",
    "            api_key=XAI_API_KEY,\n",
    "            base_url=XAI_BASE_URL,\n",
    "        )\n",
    "        print(f\"✅ OpenAI client initialized successfully for Base URL: {XAI_BASE_URL}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error initializing OpenAI client: {e}\", file=sys.stderr)\n",
    "else:\n",
    "    print(\"⚠️ Error: Environment variable 'XAI_API_KEY' not set.\", file=sys.stderr)\n",
    "\n",
    "# --- Tokenizer Initialization (Added) ---\n",
    "tokenizer = None\n",
    "try:\n",
    "    # Using cl100k_base as a common default. THIS MAY NOT BE ACCURATE FOR GROK.\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    # Or try: tokenizer = tiktoken.encoding_for_model(\"gpt-4\") # Might be closer? Needs testing.\n",
    "    print(\"✅ tiktoken tokenizer ('cl100k_base') initialized for token estimation.\")\n",
    "    print(\"   ℹ️ Note: Token counts are estimates; Grok's internal counting may differ.\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Warning: Failed to initialize tiktoken tokenizer: {e}. Token counting will be skipped.\", file=sys.stderr)\n",
    "\n",
    "\n",
    "# Display warning if client initialization failed\n",
    "if not client:\n",
    "     display(Markdown(\"<font color='red'>**Warning:** API Client not initialized. Translation will not work. Ensure `XAI_API_KEY` is set correctly in your environment.</font>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5be2cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined with updated translation prompt.\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 3: Helper Functions ---\n",
    "# Includes updated system prompt in translate_via_grok_api\n",
    "\n",
    "def count_tokens(text):\n",
    "    \"\"\"Estimates token count for a given text using the initialized tokenizer.\"\"\"\n",
    "    if not tokenizer or not text:\n",
    "        return 0\n",
    "    try:\n",
    "        return len(tokenizer.encode(text))\n",
    "    except Exception as e:\n",
    "        # Avoid crashing if encoding fails for some reason\n",
    "        print(f\"⚠️ Warning: tiktoken failed to encode text snippet: {e}\", file=sys.stderr)\n",
    "        return 0\n",
    "\n",
    "def translate_via_grok_api(text_to_translate):\n",
    "    \"\"\"\n",
    "    Uses Grok API to translate text, applying specific rules, and estimates token usage.\n",
    "\n",
    "    Rules:\n",
    "    1. Output only the translation, no extra explanations.\n",
    "    2. Do not translate literature citations/references, keep original format.\n",
    "\n",
    "    Args:\n",
    "        text_to_translate (str): The text chunk to translate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (translated_text, prompt_tokens, completion_tokens)\n",
    "               Returns (None, estimated_prompt_tokens, 0) on API failure.\n",
    "               Returns (None, 0, 0) if client not initialized.\n",
    "    \"\"\"\n",
    "    if not client:\n",
    "        print(\"❌ Error: API client is not initialized.\", file=sys.stderr)\n",
    "        return None, 0, 0\n",
    "\n",
    "    # --- Updated System Prompt ---\n",
    "    system_prompt_content = \"\"\"Act as a direct translator. Your task is to translate the provided Markdown text into Simplified Chinese according to these rules:\n",
    "1.  **Direct Output:** Provide ONLY the translated Simplified Chinese text. Do NOT include any introductory phrases, explanations, apologies, or concluding remarks like \"Here is the translation:\", \"以下是翻译内容：\", etc. Just the translation itself.\n",
    "2.  **Preserve Formatting:** Maintain all original Markdown formatting (headings `##`, lists `* - 1.`, bold `**`, italics `*`, code blocks ```` ``` ``, inline code `` ` ``, etc.) precisely.\n",
    "3.  **Do Not Translate Citations/References:** Identify text that functions as a literature citation or bibliography/reference list entry. Keep the original text and formatting for these parts *without translating them*. This applies to:\n",
    "    * In-text citations (e.g., `(Author, 2023)`, `[Author et al., 2023]`, `[1]`, `Author (2023)`).\n",
    "    * Items in reference lists (often lines starting with `*`, `-`, or `1.` followed by author names, year, title, journal/publisher, DOI, URL, etc.). Example: `* Author, A. B. (2023). Title of work. *Journal Name*, 10(2), 123-145. https://doi.org/xxxx` should remain exactly as is.\n",
    "4.  **Translate Everything Else:** All other text content should be translated accurately into Simplified Chinese.\n",
    "Ensure the final output flows naturally, integrating the untranslated citations correctly within the translated surrounding text.\"\"\"\n",
    "\n",
    "    # Prepare messages with the new system prompt\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt_content},\n",
    "        {\"role\": \"user\", \"content\": text_to_translate}\n",
    "    ]\n",
    "\n",
    "    # Estimate prompt tokens\n",
    "    prompt_tokens = 0\n",
    "    if tokenizer:\n",
    "        # Estimate tokens for messages, adding potential overhead\n",
    "        prompt_tokens += count_tokens(messages[0]['content']) + 4 # System prompt tokens + overhead\n",
    "        prompt_tokens += count_tokens(messages[1]['content']) + 4 # User content tokens + overhead\n",
    "\n",
    "    print(f\"   ➡️ Calling Grok API (Model: {XAI_MODEL}, Est. Prompt Tokens: {prompt_tokens})...\")\n",
    "\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=XAI_MODEL,\n",
    "            messages=messages,\n",
    "            temperature=0.2, # Lower temperature might help follow instructions strictly\n",
    "            # Consider adding stop sequences if needed, though the prompt should handle \"no explanation\"\n",
    "        )\n",
    "        translated_text = completion.choices[0].message.content\n",
    "\n",
    "        # Estimate completion tokens\n",
    "        completion_tokens = 0\n",
    "        if translated_text and tokenizer:\n",
    "            completion_tokens = count_tokens(translated_text)\n",
    "\n",
    "        print(f\"   ✅ API call successful (Est. Completion Tokens: {completion_tokens}).\")\n",
    "        # Basic check to remove potential leading/trailing explanation artifacts if prompt isn't perfectly followed\n",
    "        # (More robust post-processing could be added if needed)\n",
    "        cleaned_translation = translated_text.strip() if translated_text else None\n",
    "        return cleaned_translation, prompt_tokens, completion_tokens\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error calling Grok API: {e}\", file=sys.stderr)\n",
    "        return None, prompt_tokens, 0 # Return prompt tokens estimated before failure\n",
    "\n",
    "\n",
    "# --- Other Helper Functions (split_into_paragraphs, process_markdown_file) remain the same ---\n",
    "# Make sure process_markdown_file still correctly calls this updated translate_via_grok_api\n",
    "# and handles the returned tuple (translation_result, p_tokens, c_tokens).\n",
    "# (No changes needed to process_markdown_file from the previous version with token counting)\n",
    "\n",
    "def split_into_paragraphs(text):\n",
    "    \"\"\"Splits text into paragraphs based on blank lines.\"\"\"\n",
    "    paragraphs = re.split(r'\\n\\s*\\n', text.strip())\n",
    "    return [p for p in paragraphs if p.strip()]\n",
    "\n",
    "def process_markdown_file(input_filepath, output_filepath):\n",
    "    \"\"\"\n",
    "    Reads, translates (in chunks with progress bar), estimates tokens, and writes the Markdown file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (total_prompt_tokens, total_completion_tokens)\n",
    "    \"\"\"\n",
    "    total_prompt_tokens = 0\n",
    "    total_completion_tokens = 0\n",
    "\n",
    "    if not client:\n",
    "        print(\"❌ Error: Cannot process file because API client is not initialized.\", file=sys.stderr)\n",
    "        return total_prompt_tokens, total_completion_tokens # Return zero tokens\n",
    "\n",
    "    print(f\"Reading input file: {input_filepath}\")\n",
    "    try:\n",
    "        with open(input_filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: Input file not found: {input_filepath}\", file=sys.stderr)\n",
    "        return total_prompt_tokens, total_completion_tokens\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error reading file '{input_filepath}': {e}\", file=sys.stderr)\n",
    "        return total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "    paragraphs = split_into_paragraphs(content)\n",
    "    if not paragraphs:\n",
    "        print(f\"⚠️ Warning: Input file '{input_filepath}' is empty or has no valid paragraphs.\")\n",
    "        try:\n",
    "            with open(output_filepath, 'w', encoding='utf-8') as f: f.write(\"\")\n",
    "            print(f\"   Empty output file created: {output_filepath}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error writing empty output file '{output_filepath}': {e}\", file=sys.stderr)\n",
    "        return total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "    translated_chunks = []\n",
    "    total_paragraphs = len(paragraphs)\n",
    "    num_chunks = (total_paragraphs + CHUNK_SIZE - 1) // CHUNK_SIZE\n",
    "\n",
    "    print(f\"Starting translation of {total_paragraphs} paragraphs in {num_chunks} chunks...\")\n",
    "\n",
    "    # --- Initialize Progress Bar ---\n",
    "    pbar = tqdm(total=total_paragraphs, unit=\"paragraph\", desc=\"Translating\")\n",
    "\n",
    "    for i in range(0, total_paragraphs, CHUNK_SIZE):\n",
    "        chunk_paragraphs = paragraphs[i:i + CHUNK_SIZE]\n",
    "        text_chunk_to_translate = \"\\n\\n\".join(chunk_paragraphs)\n",
    "        chunk_index = (i // CHUNK_SIZE) + 1\n",
    "        start_para = i + 1\n",
    "        end_para = min(i + CHUNK_SIZE, total_paragraphs)\n",
    "\n",
    "        pbar.set_description(f\"Chunk {chunk_index}/{num_chunks} (Paras {start_para}-{end_para})\")\n",
    "\n",
    "        translated_chunk_text = None\n",
    "        prompt_tokens_chunk = 0\n",
    "        completion_tokens_chunk = 0\n",
    "        final_prompt_tokens_for_chunk = 0\n",
    "\n",
    "        for attempt in range(MAX_RETRIES):\n",
    "            translation_result, p_tokens, c_tokens = translate_via_grok_api(text_chunk_to_translate) # Calls the updated function\n",
    "\n",
    "            if attempt == 0:\n",
    "                final_prompt_tokens_for_chunk = p_tokens\n",
    "\n",
    "            if translation_result is not None:\n",
    "                translated_chunk_text = translation_result\n",
    "                completion_tokens_chunk = c_tokens\n",
    "                break\n",
    "            else:\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    print(f\"      Waiting {API_RETRY_DELAY}s before retrying...\")\n",
    "                    time.sleep(API_RETRY_DELAY)\n",
    "\n",
    "        if translated_chunk_text is not None:\n",
    "            translated_chunks.append(translated_chunk_text)\n",
    "        else:\n",
    "            error_marker = f\"[Grok API Translation Failed - Chunk {chunk_index}]\"\n",
    "            print(f\"   ❌ Error: Chunk {chunk_index} failed after {MAX_RETRIES} attempts. Original text kept.\", file=sys.stderr)\n",
    "            translated_chunks.append(f\"{error_marker}\\n\\n{text_chunk_to_translate}\")\n",
    "            completion_tokens_chunk = 0\n",
    "\n",
    "        total_prompt_tokens += final_prompt_tokens_for_chunk\n",
    "        total_completion_tokens += completion_tokens_chunk\n",
    "        pbar.update(len(chunk_paragraphs))\n",
    "\n",
    "    pbar.close()\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    # --- Write Output File ---\n",
    "    final_translated_content = \"\\n\\n\".join(translated_chunks)\n",
    "    print(f\"Writing translated content to: {output_filepath}\")\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            f.write(final_translated_content)\n",
    "        print(\"✅ Translation process complete!\")\n",
    "        display(Markdown(f\"**Success!** Translated file saved to: `{output_filepath}`\"))\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error writing output file '{output_filepath}': {e}\", file=sys.stderr)\n",
    "        display(Markdown(f\"<font color='red'>**Error:** Failed to write output file: `{output_filepath}`</font>\"))\n",
    "\n",
    "    return total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "\n",
    "print(\"Helper functions defined with updated translation prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "13d6975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Input File:  'D:\\book-CO2_enhanced_gas\\document\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\full.md'\n",
      "Output File: 'D:\\book-CO2_enhanced_gas\\document\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\full_translated_zh.md'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 4: Specify File Paths ---\n",
    "\n",
    "# <<<---------------------------------------------------------->>>\n",
    "# <<<--- !!! DEFINE YOUR INPUT AND OUTPUT FILE PATHS HERE !!! --->>>\n",
    "# <<<---------------------------------------------------------->>>\n",
    "\n",
    "# Option 1: Specify both input and output paths directly\n",
    "input_filepath = \"D:\\\\book-CO2_enhanced_gas\\\\document\\\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\\\full.md\"  # <--- REQUIRED: Set path to your input Markdown file\n",
    "output_filepath = \"D:\\\\book-CO2_enhanced_gas\\\\document\\\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\\\full_translated_zh.md\" # <--- OPTIONAL: Set desired output path\n",
    "\n",
    "# Option 2: Specify only input, let output be derived automatically\n",
    "# input_filepath = \"another_doc.md\" # <--- REQUIRED\n",
    "# output_filepath = \"\" # <--- Leave empty or comment out to auto-derive\n",
    "\n",
    "# --- Auto-generate output path if not specified ---\n",
    "if not output_filepath:\n",
    "    if input_filepath:\n",
    "        base, ext = os.path.splitext(input_filepath)\n",
    "        output_filepath = f\"{base}_zh_translated{ext}\"\n",
    "        print(f\"Output filepath not specified, auto-generated: {output_filepath}\")\n",
    "    else:\n",
    "        print(\"⚠️ Warning: Input filepath is not set.\", file=sys.stderr)\n",
    "        output_filepath = \"default_output_translated.md\" # Default fallback\n",
    "\n",
    "# --- Display configured paths ---\n",
    "print(\"-\" * 50)\n",
    "print(f\"Input File:  '{input_filepath}'\")\n",
    "print(f\"Output File: '{output_filepath}'\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfad4cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting translation process...\n",
      "Using Model: grok-3-mini, Chunk Size: 3 paragraphs\n",
      "--------------------------------------------------\n",
      "Checks passed. Executing translation...\n",
      "Reading input file: D:\\book-CO2_enhanced_gas\\document\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\full.md\n",
      "Starting translation of 128 paragraphs in 43 chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150f81ef225e47cf89e117bf2d5d8dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating:   0%|          | 0/128 [00:00<?, ?paragraph/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1042)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 941).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 792)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 584).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1086)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 995).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 868)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 742).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 658)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 422).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 874)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 388).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 802)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 540).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 514)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 199).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 491)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 186).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 602)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 290).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 479)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 152).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 498)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 156).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 513)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 216).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 429)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 93).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 522)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 215).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 570)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 266).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 740)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 480).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1016)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 852).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1880)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 1562).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 421)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 101).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 978)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 772).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 715)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 423).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 723)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 451).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1109)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 937).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 787)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 564).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 986)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 818).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 575)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 305).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1101)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 925).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 826)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 615).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 571)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 292).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 716)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 404).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1022)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 872).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 646)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 407).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 1013)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 872).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 806)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 581).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 449)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 113).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 979)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 825).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 588)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 322).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 766)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 578).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 500)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 213).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 365)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 28).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 423)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 102).\n",
      "   ➡️ Calling Grok API (Model: grok-3-mini, Est. Prompt Tokens: 2174)...\n",
      "   ✅ API call successful (Est. Completion Tokens: 1835).\n",
      "--------------------------------------------------\n",
      "Writing translated content to: D:\\book-CO2_enhanced_gas\\document\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\full_translated_zh.md\n",
      "✅ Translation process complete!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Success!** Translated file saved to: `D:\\book-CO2_enhanced_gas\\document\\2021-adts.202100127.pdf-e77c6731-95f5-40e9-a7f5-fba1ff52e33a\\full_translated_zh.md`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "\n",
      "--- Token Usage Estimation Summary ---\n",
      "Estimated Prompt Tokens:     33615\n",
      "Estimated Completion Tokens: 22634\n",
      "Estimated Total Tokens Used: 56249\n",
      "ℹ️ Note: Token counts are estimates using the 'cl100k_base' tokenizer via tiktoken.\n",
      "   Actual token usage by Grok may differ.\n",
      "\n",
      "Total execution time: 618.61 seconds\n",
      "\n",
      "--- Cell Execution Finished ---\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 5: Execute Translation ---\n",
    "\n",
    "print(\"Starting translation process...\")\n",
    "print(f\"Using Model: {XAI_MODEL}, Chunk Size: {CHUNK_SIZE} paragraphs\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- Pre-run Checks ---\n",
    "ready_to_run = True\n",
    "if not client:\n",
    "    print(\"❌ Error: API Client is not initialized. Cannot run.\", file=sys.stderr)\n",
    "    display(Markdown(\"<font color='red'>**Execution Halted:** API Client not ready. Check Cell 2 output and `XAI_API_KEY` environment variable.</font>\"))\n",
    "    ready_to_run = False\n",
    "\n",
    "if not input_filepath or not os.path.exists(input_filepath):\n",
    "    print(f\"❌ Error: Input file not found or not specified: '{input_filepath}'\", file=sys.stderr)\n",
    "    display(Markdown(f\"<font color='red'>**Execution Halted:** Input file path is invalid. Check Cell 4.</font>\"))\n",
    "    ready_to_run = False\n",
    "\n",
    "if input_filepath and output_filepath and os.path.abspath(input_filepath) == os.path.abspath(output_filepath):\n",
    "    print(\"❌ Error: Input and output file paths cannot be the same.\", file=sys.stderr)\n",
    "    display(Markdown(f\"<font color='red'>**Execution Halted:** Input and Output paths point to the same file. Specify a different output path in Cell 4.</font>\"))\n",
    "    ready_to_run = False\n",
    "\n",
    "# --- Run Translation if Checks Pass ---\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "\n",
    "if ready_to_run:\n",
    "    print(\"Checks passed. Executing translation...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Call the processing function which now returns token counts\n",
    "    total_prompt_tokens, total_completion_tokens = process_markdown_file(input_filepath, output_filepath)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"-\" * 50) # Separator after process function finishes\n",
    "\n",
    "    # --- Display Token Usage Summary ---\n",
    "    print(\"\\n--- Token Usage Estimation Summary ---\")\n",
    "    print(f\"Estimated Prompt Tokens:     {total_prompt_tokens}\")\n",
    "    print(f\"Estimated Completion Tokens: {total_completion_tokens}\")\n",
    "    print(f\"Estimated Total Tokens Used: {total_prompt_tokens + total_completion_tokens}\")\n",
    "    if not tokenizer:\n",
    "         print(\"⚠️ Token counting was skipped due to tokenizer initialization failure.\")\n",
    "    else:\n",
    "         print(\"ℹ️ Note: Token counts are estimates using the 'cl100k_base' tokenizer via tiktoken.\")\n",
    "         print(\"   Actual token usage by Grok may differ.\")\n",
    "\n",
    "    print(f\"\\nTotal execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n--- Execution Halted due to errors ---\")\n",
    "\n",
    "print(\"\\n--- Cell Execution Finished ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
